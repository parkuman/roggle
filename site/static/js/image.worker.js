/* eslint-disable no-undef */
/**
 * This file is a Web Worker Responsible for loading and
 * running OpenCV and TensorFlow commands in a worker thread
 */

const paths = {
	openCvWasm: "./opencv/wasm/opencv.js",
	openCvThreads: "./opencv/threads/opencv.js",
	openCvSimd: "./opencv/wasm/opencv.js", // TODO: use SIMD
	openCvThreadsSimd: "./opencv/threads_simd/opencv.js",
	openCvWasmFeatureDetect: "./opencv/wasm-feature-detect.js",
	tfjs: "./tensorflow/tfjs.min.js",
	tfjsWasmBackend: "./tensorflow/tfjs-backend-wasm.min.js",
	tfjsWasm: "./tensorflow/tfjs-backend-wasm.wasm",
	tfjsWasmSimd: "./tensorflow/tfjs-backend-wasm-simd.wasm",
	tfjsWasmThreadedSimd: "./tensorflow/tfjs-backend-wasm-threaded-simd.wasm",
};

const CLASSES = {
	0: "a",
	1: "b",
	2: "c",
	3: "d",
	4: "e",
	5: "f",
	6: "g",
	7: "h",
	8: "i",
	9: "j",
	10: "k",
	11: "l",
	12: "m",
	13: "n",
	14: "o",
	15: "p",
	16: "q",
	17: "r",
	18: "s",
	19: "t",
	20: "u",
	21: "v",
	22: "w",
	23: "x",
	24: "y",
	25: "z",
}

let model;


async function initTfModel() {
	model = await tf.loadLayersModel("./tensorflow/model/model.json");
}

function distanceFromLine(linePt1, linePt2, point) {
	return (
		Math.abs(
			(linePt2.y - linePt1.y) * point.x -
			(linePt2.x - linePt1.x) * point.y +
			linePt2.x * linePt1.y -
			linePt2.y * linePt1.x,
		) / Math.pow(Math.pow(linePt2.y - linePt1.y, 2) + Math.pow(linePt2.x - linePt1.x, 2), 0.5)
	);
}

function median(sortedNumbers) {
	const middle = Math.floor(sortedNumbers.length / 2);

	if (sortedNumbers.length % 2 === 0) {
		return (sortedNumbers[middle - 1] + sortedNumbers[middle]) / 2;
	}

	return sortedNumbers[middle];
}

/**
 *  This is the code mostly generated by OpenCV for loading depending on what
 *  wasm features are enabled in the browser, just moved to a function inside here.
 *
 * @returns the path to the correct opencv script to use depending on the current browser's features
 *
 * 					simd - single instruction multiple data
 * 					multithreading - multithreading using WASM
 * 					simd + threads - browser supports both for WebAssembly
 */
async function getOpenCV() {
	let OPENCV_URL = "";
	let wasmPath = "";
	let simdPath = "";
	let threadsPath = "";
	let threadsSimdPath = "";

	if ("openCvWasm" in paths) {
		wasmPath = paths["openCvWasm"];
	}

	if ("openCvThreads" in paths) {
		threadsPath = paths["openCvThreads"];
	}

	if ("openCvSimd" in paths) {
		simdPath = paths["openCvSimd"];
	}

	if ("openCvThreadsSimd" in paths) {
		threadsSimdPath = paths["openCvThreadsSimd"];
	}

	let wasmSupported = !(typeof WebAssembly === "undefined");
	if (!wasmSupported) {
		throw new Error("The browser doesn't support WebAssembly, cannot load OpenCV");
	}

	let simdSupported = wasmSupported ? await wasmFeatureDetect.simd() : false;
	let threadsSupported = wasmSupported ? await wasmFeatureDetect.threads() : false;

	if (simdSupported && threadsSupported && threadsSimdPath != "") {
		OPENCV_URL = threadsSimdPath;
		console.log("Loading OpenCV.js with simd and threads optimization");
	} else if (simdSupported && simdPath != "") {
		if (threadsSupported && threadsSimdPath === "") {
			console.log(
				"The browser supports simd and threads, but the path of OpenCV.js with simd and threads optimization is empty",
			);
		}
		OPENCV_URL = simdPath;
		console.log("Loading OpenCV.js with simd optimization");
	} else if (threadsSupported && threadsPath != "") {
		if (simdSupported && threadsSimdPath === "") {
			console.log(
				"The browser supports simd and threads, but the path of OpenCV.js with simd and threads optimization is empty",
			);
		}
		OPENCV_URL = threadsPath;
		console.log("Loading OpenCV.js with threads optimization");
	} else if (wasmSupported && wasmPath != "") {
		if (simdSupported && threadsSupported) {
			console.log(
				"The browser supports simd and threads, but the path of OpenCV.js with simd and threads optimization is empty",
			);
		}

		if (simdSupported) {
			console.log(
				"The browser supports simd optimization, but the path of OpenCV.js with simd optimization is empty",
			);
		}

		if (threadsSupported) {
			console.log(
				"The browser supports threads optimization, but the path of OpenCV.js with threads optimization is empty",
			);
		}

		OPENCV_URL = wasmPath;
		console.log("Loading OpenCV.js for wasm");
	} else if (wasmSupported) {
		console.log("The browser supports wasm, but the path of OpenCV.js for wasm is empty");
	}

	if (OPENCV_URL === "") {
		throw new Error("No available OpenCV.js, please check your paths");
	}

	return OPENCV_URL;
}

function imageProcessing({ msg, data, debug }) {
	const img = cv.matFromImageData(data);

	// ========================== THRESHOLDING + FILTERING ==========================
	cv.cvtColor(img, img, cv.COLOR_BGR2GRAY);
	const originalGrayscaleImg = img.clone();
	const workingImg = img.clone();
	postMessage({ msg: "msg", payload: "grayscale" });

	// blur then sharpen to enhance edges
	cv.medianBlur(img, img, 7);

	const sharpeningKernel = cv.matFromArray(3, 3, cv.CV_32FC1, [0, -1, 0, -1, 5, -1, 0, -1, 0]); // https://en.wikipedia.org/wiki/Kernel_(image_processing)#Details
	cv.filter2D(img, img, -1, sharpeningKernel);

	// binary threshold the image to get extremes
	cv.threshold(img, img, 190, 255, cv.THRESH_BINARY);

	// use rectangular morphology to filter out extra tidbits / small unwanted thresholded pieces
	const rectangularKernel = cv.getStructuringElement(cv.MORPH_RECT, new cv.Size(3, 3));
	cv.filter2D(img, img, -1, rectangularKernel);
	cv.morphologyEx(img, img, cv.MORPH_OPEN, rectangularKernel, new cv.Point(-1, 1), 2);

	// ========================== CONTOURS ==========================
	const contours = new cv.MatVector();
	const hierarchy = new cv.Mat();
	// const contourImgBuffer = cv.Mat.zeros(img.rows, img.cols, cv.CV_8UC3);
	const color = new cv.Scalar(255, 255, 255);

	const minArea = 1000; // lower bound on acceptable contour areas (this is very small)
	const maxArea = 20000; // upper bound on acceptable contour areas (this is huge, bigger than any letter block should be)
	const contourData = []; // array to hold an object containing a contour, its area, and its bounding rectangle


	// find all contours in the image
	cv.findContours(img, contours, hierarchy, cv.RETR_CCOMP, cv.CHAIN_APPROX_SIMPLE);

	// get all contours and contour areas areas and add them to an array
	for (let i = 0; i < contours.size(); i++) {
		// uncomment to see all the contours identified
		//cv.drawContours(contourImgBuffer, contours, i, color, 1, cv.LINE_8, hierarchy, 0);

		const contour = contours.get(i);
		const area = cv.contourArea(contour);

		// if within an acceptable area, save the contour and its area to an object and add it to the list of contours we think are decent enough to continue with
		if (area > minArea && area < maxArea) {
			contourData.push({
				contour,
				area,
			});
		}
	}

	const letterAreaDeviation = 3000; // acceptable range above and below median area of a letter contour
	const letterData = []; // array to hold on object containing a letter block's midpoint point and it's bounding box

	// sort the contours by area and find the median value. we will use the median plus a range to try to filter out outlier contours
	contourData.sort((c1, c2) => c1.area - c2.area);
	const medianArea = median(contourData.map((c) => c.area));

	// const goodContours = new cv.MatVector();

	// loop over each contour and attempt to find the ones that are within an acceptable range of the median value
	contourData.forEach((contour) => {
		// only keep contours within a certain area range of the median
		if (
			contour.area < medianArea + letterAreaDeviation &&
			contour.area > medianArea - letterAreaDeviation
		) {
			// goodContours.push_back(contour.contour);
			const boundingRect = cv.boundingRect(contour.contour);
			const rectTopLeftX = boundingRect.x;
			const rectTopLeftY = boundingRect.y;
			const rectBottomRightX = rectTopLeftX + boundingRect.width;
			const rectBottomRightY = rectTopLeftY + boundingRect.height;

			letterData.push({
				x: Math.floor(rectTopLeftX + boundingRect.width / 2),
				y: Math.floor(rectTopLeftY + boundingRect.height / 2),
				boundingBox: {
					x: rectTopLeftX,
					y: rectTopLeftY,
					width: boundingRect.width,
					height: boundingRect.height,
				},
			});

			let topLeft = new cv.Point(rectTopLeftX, rectTopLeftY);
			let bottomRight = new cv.Point(rectBottomRightX, rectBottomRightY);
			cv.rectangle(workingImg, topLeft, bottomRight, color, 2, cv.LINE_AA, 0);
		}
	});

	postMessage({ msg: "msg", payload: "done contours" });


	// draw only the good contours (ideally this should only have the letter boxes at this point)
	// for (let i = 0; i < goodContours.size(); i++) {
	// 	cv.drawContours(contourImgBuffer, goodContours, i, color, 1, cv.LINE_8, hierarchy, 0);
	// }

	// ========================== ROW FINDING ALGORITHM ==========================
	let pointsToSearch = letterData;
	let sortedPoints = [];
	let maxRowSize = null;

	cv.cvtColor(workingImg, workingImg, cv.COLOR_GRAY2BGR); // convert from grayscale to rgb so we can add colored info ontop of the image
	// uncomment to see all the midpoints of all the letters identified
	// pointsToSearch.forEach((pt) => {
	// 	cv.circle(workingImg, new cv.Point(pt.x, pt.y), 3, [255, 0, 0, 255], 3);
	// });

	while (pointsToSearch.length > 0) {
		const boundingPointsSum = pointsToSearch
			.map((pt) => ({ x: pt.x, y: pt.y, sum: pt.x + pt.y }))
			.sort((a, b) => a.sum - b.sum);

		const boundingPointsDiff = pointsToSearch
			.map((pt) => ({ x: pt.x, y: pt.y, diff: pt.x - pt.y }))
			.sort((a, b) => a.diff - b.diff);

		const lastIdx = pointsToSearch.length - 1;
		const topLeft = new cv.Point(boundingPointsSum[0].x, boundingPointsSum[0].y);
		const topRight = new cv.Point(boundingPointsDiff[lastIdx].x, boundingPointsDiff[lastIdx].y);

		const pointsInRow = [];
		const remainingPointsToSearch = [];

		// let rowColor = [
		// 	Math.round(Math.random() * 255),
		// 	Math.round(Math.random() * 255),
		// 	Math.round(Math.random() * 255),
		// 	255,
		// ];
		pointsToSearch.forEach((pt) => {
			const distance = distanceFromLine(topLeft, topRight, pt);

			if (distance < 50) {
				pointsInRow.push(pt);

				// uncomment to display unique colored dot on each box in the row
				// cv.circle(workingImg, new cv.Point(pt.x, pt.y), 3, rowColor, 3);
			} else {
				remainingPointsToSearch.push(pt);
			}
		});

		// row size validation
		if (maxRowSize === null) {
			// first run through, need to set an inital row size
			maxRowSize = pointsInRow.length;
		} else if (pointsInRow.length !== maxRowSize) {
			// subsequent run throughs, if the row sizes don't match we can't continue (we need an NxM board here folks!!!)
			postMessage({
				msg,
				payload: "Error: Could not extract an NxM board from image",
			});
			return;
		}

		// reset the points to search to only be the ones that weren't found to be in the current row
		pointsToSearch = remainingPointsToSearch;

		// add the items found in the row sorted by x to the overall sorted points array
		sortedPoints.push(...pointsInRow.sort((pt1, pt2) => pt1.x - pt2.x));

		// TODO: break out of here if it isn't getting smaller
	}


	// ========================== EXTRACT SORTED LETTERS FOR PREDICTION ==========================
	const lettersCropped = []; // array to hold a list of all letter image data cropped regions of interest, this array will be passed into the prediction step
	const IMG_SIZE = 48;			 // our classification model takes in images that are 48x48 pixels
	const IMG_AREA = IMG_SIZE * IMG_SIZE;

	sortedPoints.forEach((pt) => {
		const boundingRect = new cv.Rect(
			pt.boundingBox.x,
			pt.boundingBox.y,
			pt.boundingBox.width,
			pt.boundingBox.height,
		);

		// crop letter and resize them all to the same size
		const roi = originalGrayscaleImg.roi(boundingRect);
		cv.resize(roi, roi, new cv.Size(IMG_SIZE, IMG_SIZE), 0, 0, cv.INTER_AREA);
		lettersCropped.push(roi);

		// uncomment to print numbered index beside each point
		// cv.putText(
		// 	workingImg,
		// 	idx.toString(),
		// 	new cv.Point(pt.x + 10, pt.y + 10),
		// 	cv.FONT_HERSHEY_SIMPLEX,
		// 	1,
		// 	[255, 0, 0, 255],
		// 	2,
		// 	cv.LINE_AA,
		// );
	});

	// ========================== PREDICTION ==========================
	const TOTAL_LETTERS = lettersCropped.length;
	const inputTensorArr = new Float32Array(
		IMG_AREA * TOTAL_LETTERS
	);

	// insert the raw opencv image data into the new Float32Array we made
	lettersCropped.forEach((croppedLetter, i) => {
		inputTensorArr.set(croppedLetter.data, i * IMG_AREA);
	})

	const testTensor = tf.tensor2d(inputTensorArr, [TOTAL_LETTERS, IMG_AREA]);
	const reshaped = testTensor.reshape([TOTAL_LETTERS, IMG_SIZE, IMG_SIZE, 1]);

	const predictions = model.predict(reshaped).dataSync();

	let predictedBoard = "";
	for (let i = 0; i < TOTAL_LETTERS; i++) {
		const letterPredictionsArr = Array.from(predictions).slice(i * 26, i * 26 + 26);
		const mostLikelyLetter = Math.max(...letterPredictionsArr);
		const letterIndex = letterPredictionsArr.indexOf(mostLikelyLetter);
		predictedBoard += CLASSES[letterIndex];

		// if we are at the end of a row, add a space to indicate a new row in the board
		if ((i + 1) % maxRowSize === 0) {
			predictedBoard += " ";
		}
	}

	// by now, predictedBoard should look something like "abcd efgh ijlm opqr" for a 4x4 board for example
	postMessage({ msg, payload: predictedBoard });

	// ========================== RETURN + CLEANUP ==========================
	// postMessage({ msg, payload: lettersCropped });
	// postMessage({ msg, payload: imageDataFromMat(workingImg) });

	// contourImgBuffer.delete();
	contours.delete();
	hierarchy.delete();
	img.delete();
	workingImg.delete();
	originalGrayscaleImg.delete();
}

/**
 * This function converts again from cv.Mat to ImageData
 */
function imageDataFromMat(mat) {
	// converts the mat type to cv.CV_8U
	const img = new cv.Mat();
	const depth = mat.type() % 8;
	const scale = depth <= cv.CV_8S ? 1.0 : depth <= cv.CV_32S ? 1.0 / 256.0 : 255.0;
	const shift = depth === cv.CV_8S || depth === cv.CV_16S ? 128.0 : 0.0;
	mat.convertTo(img, cv.CV_8U, scale, shift);

	// converts the img type to cv.CV_8UC4
	switch (img.type()) {
		case cv.CV_8UC1:
			cv.cvtColor(img, img, cv.COLOR_GRAY2RGBA);
			break;
		case cv.CV_8UC3:
			cv.cvtColor(img, img, cv.COLOR_RGB2RGBA);
			break;
		case cv.CV_8UC4:
			break;
		default:
			throw new Error("Bad number of channels (Source image must have 1, 3 or 4 channels)");
	}
	const clampedArray = new ImageData(new Uint8ClampedArray(img.data), img.cols, img.rows);
	return clampedArray;
}

onmessage = async function (e) {
	switch (e.data.msg) {
		case "load": {
			try {
				// load opencv depending on wasm features available
				self.importScripts(paths.openCvWasmFeatureDetect);
				const opencvScript = await getOpenCV();
				self.importScripts(opencvScript);
				cv = await cv;

				// load tensorflow scripts
				self.importScripts(paths.tfjs);
				self.importScripts(paths.tfjsWasmBackend);
				tf.wasm.setWasmPaths({
					"tfjs-backend-wasm.wasm": paths.tfjsWasm,
					"tfjs-backend-wasm-simd.wasm": paths.tfjsWasmSimd,
					"tfjs-backend-wasm-threaded-simd.wasm": paths.tfjsWasmThreadedSimd,
				});
				tf.setBackend("wasm");
				initTfModel();
				await tf.ready();
			} catch (err) {
				throw new Error(err);
			}

			postMessage({ msg: e.data.msg });
			break;
		}
		case "imageProcessing":
			return imageProcessing(e.data);
		default:
			break;
	}
};
