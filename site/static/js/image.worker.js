/* eslint-disable no-undef */
/**
 * This file is a Web Worker Responsible for loading and
 * running OpenCV commands off the main thread
 */

const paths = {
	openCvWasm: "./opencv/wasm/opencv.js",
	openCvThreads: "./opencv/threads/opencv.js",
	openCvSimd: "./opencv/wasm/opencv.js", // TODO: use SIMD
	openCvThreadsSimd: "./opencv/threads_simd/opencv.js",
	openCvWasmFeatureDetect: "./opencv/wasm-feature-detect.js",
	tfjs: "./tensorflow/tfjs.min.js",
	tfjsWasmBackend: "./tensorflow/tfjs-backend-wasm.min.js",
	tfjsWasm: "./tensorflow/tfjs-backend-wasm.wasm",
	tfjsWasmSimd: "./tensorflow/tfjs-backend-wasm-simd.wasm",
	tfjsWasmThreadedSimd: "./tensorflow/tfjs-backend-wasm-threaded-simd.wasm"
};

function distanceFromLine(linePt1, linePt2, point) {
	return (
		Math.abs(
			(linePt2.y - linePt1.y) * point.x -
				(linePt2.x - linePt1.x) * point.y +
				linePt2.x * linePt1.y -
				linePt2.y * linePt1.x
		) / Math.pow(Math.pow(linePt2.y - linePt1.y, 2) + Math.pow(linePt2.x - linePt1.x, 2), 0.5)
	);
}

/**
 *  This is the code mostly generated by OpenCV for loading depending on what
 *  wasm features are enabled in the browser, just moved to a function inside here.
 *
 * @returns the path to the correct opencv script to use depending on the current browser's features
 *
 * 					simd - single instruction multiple data
 * 					multithreading - multithreading using WASM
 * 					simd + threads - browser supports both for WebAssembly
 */
async function getOpenCV() {
	let OPENCV_URL = "";
	let wasmPath = "";
	let simdPath = "";
	let threadsPath = "";
	let threadsSimdPath = "";

	if ("openCvWasm" in paths) {
		wasmPath = paths["openCvWasm"];
	}

	if ("openCvThreads" in paths) {
		threadsPath = paths["openCvThreads"];
	}

	if ("openCvSimd" in paths) {
		simdPath = paths["openCvSimd"];
	}

	if ("openCvThreadsSimd" in paths) {
		threadsSimdPath = paths["openCvThreadsSimd"];
	}

	let wasmSupported = !(typeof WebAssembly === "undefined");
	if (!wasmSupported) {
		throw new Error("The browser doesn't support WebAssembly, cannot load OpenCV");
	}

	let simdSupported = wasmSupported ? await wasmFeatureDetect.simd() : false;
	let threadsSupported = wasmSupported ? await wasmFeatureDetect.threads() : false;

	if (simdSupported && threadsSupported && threadsSimdPath != "") {
		OPENCV_URL = threadsSimdPath;
		console.log("Loading OpenCV.js with simd and threads optimization");
	} else if (simdSupported && simdPath != "") {
		if (threadsSupported && threadsSimdPath === "") {
			console.log(
				"The browser supports simd and threads, but the path of OpenCV.js with simd and threads optimization is empty"
			);
		}
		OPENCV_URL = simdPath;
		console.log("Loading OpenCV.js with simd optimization");
	} else if (threadsSupported && threadsPath != "") {
		if (simdSupported && threadsSimdPath === "") {
			console.log(
				"The browser supports simd and threads, but the path of OpenCV.js with simd and threads optimization is empty"
			);
		}
		OPENCV_URL = threadsPath;
		console.log("Loading OpenCV.js with threads optimization");
	} else if (wasmSupported && wasmPath != "") {
		if (simdSupported && threadsSupported) {
			console.log(
				"The browser supports simd and threads, but the path of OpenCV.js with simd and threads optimization is empty"
			);
		}

		if (simdSupported) {
			console.log(
				"The browser supports simd optimization, but the path of OpenCV.js with simd optimization is empty"
			);
		}

		if (threadsSupported) {
			console.log(
				"The browser supports threads optimization, but the path of OpenCV.js with threads optimization is empty"
			);
		}

		OPENCV_URL = wasmPath;
		console.log("Loading OpenCV.js for wasm");
	} else if (wasmSupported) {
		console.log("The browser supports wasm, but the path of OpenCV.js for wasm is empty");
	}

	if (OPENCV_URL === "") {
		throw new Error("No available OpenCV.js, please check your paths");
	}

	return OPENCV_URL;
}

function imageProcessing({ msg, data }) {
	const img = cv.matFromImageData(data);

	// ========================== THRESHOLDING + FILTERING ==========================
	cv.cvtColor(img, img, cv.COLOR_BGR2GRAY);
	let grayscaleImg = img.clone();

	// blur then sharpen to enhance edges
	cv.medianBlur(img, img, 7);
	const sharpeningKernel = cv.matFromArray(3, 3, cv.CV_32FC1, [0, -1, 0, -1, 5, -1, 0, -1, 0]);
	cv.filter2D(img, img, -1, sharpeningKernel);

	// binary threshold the image to get extremes
	cv.threshold(img, img, 190, 255, cv.THRESH_BINARY);

	// use rectangular morphology to filter out extra tidbits / small unwanted thresholded pieces
	const rectangularKernel = cv.getStructuringElement(cv.MORPH_RECT, new cv.Size(3, 3));
	cv.filter2D(img, img, -1, rectangularKernel);
	cv.morphologyEx(img, img, cv.MORPH_OPEN, rectangularKernel, new cv.Point(-1, 1), 2);

	// ========================== CONTOURS ==========================
	const contours = new cv.MatVector();
	const hierarchy = new cv.Mat();
	// const contourImgBuffer = cv.Mat.zeros(img.rows, img.cols, cv.CV_8UC3);
	const color = new cv.Scalar(255, 0, 0);
	const minArea = 3000;
	const maxArea = 12000;
	const letters = [];
	const boundingPoints = [];
	const midpoints = [];

	// find all contours in the image
	cv.findContours(img, contours, hierarchy, cv.RETR_CCOMP, cv.CHAIN_APPROX_SIMPLE);

	for (let i = 0; i < contours.size(); ++i) {
		const contour = contours.get(i);
		const area = cv.contourArea(contour);

		// only keep contours within a certain area range (the boggle letter blocks have an area of approximately 8000 for this image size)
		if (area < maxArea && area > minArea) {
			// cv.drawContours(contourImgBuffer, contours, i, color, 1, cv.LINE_8, hierarchy, 0);

			const boundingRect = cv.boundingRect(contour);
			const rectTopLeftX = boundingRect.x;
			const rectTopLeftY = boundingRect.y;
			const rectBottomRightX = rectTopLeftX + boundingRect.width;
			const rectBottomRightY = rectTopLeftY + boundingRect.height;

			midpoints.push({
				x: Math.floor(rectTopLeftX + boundingRect.width / 2),
				y: Math.floor(rectTopLeftY + boundingRect.height / 2)
			});

			boundingPoints.push({
				x: rectTopLeftX,
				y: rectTopLeftY
			}); // top left
			boundingPoints.push({ x: rectTopLeftX, y: rectTopLeftY + boundingRect.height }); // bottom left
			boundingPoints.push({ x: rectTopLeftX + boundingRect.width, y: rectTopLeftY }); // top right
			boundingPoints.push({ x: rectBottomRightX, y: rectBottomRightY }); // bottom right
			letters.push(grayscaleImg.roi(boundingRect));

			let topLeft = new cv.Point(rectTopLeftX, rectTopLeftY);
			let bottomRight = new cv.Point(rectBottomRightX, rectBottomRightY);
			cv.rectangle(grayscaleImg, topLeft, bottomRight, color, 2, cv.LINE_AA, 0);
		}
	}

	let pointsToSearch = midpoints;

	cv.cvtColor(grayscaleImg, grayscaleImg, cv.COLOR_GRAY2BGR);
	midpoints.forEach((pt) => {
		cv.circle(grayscaleImg, new cv.Point(pt.x, pt.y), 3, [255, 0, 0, 255], 3);
	});

	while (pointsToSearch.length > 0) {
		const boundingPointsSum = pointsToSearch
			.map((pt) => ({ x: pt.x, y: pt.y, sum: pt.x + pt.y }))
			.sort((a, b) => {
				if (a.sum < b.sum) return -1;
				if (a.sum > b.sum) return 1;
				return 0;
			});

		const boundingPointsDiff = pointsToSearch
			.map((pt) => ({ x: pt.x, y: pt.y, diff: pt.x - pt.y }))
			.sort((a, b) => {
				if (a.diff < b.diff) return -1;
				if (a.diff > b.diff) return 1;
				return 0;
			});

		const lastIdx = pointsToSearch.length - 1;
		const topLeft = new cv.Point(boundingPointsSum[0].x, boundingPointsSum[0].y);
		const topRight = new cv.Point(boundingPointsDiff[lastIdx].x, boundingPointsDiff[lastIdx].y);

		// const bottomLeft = new cv.Point(boundingPointsDiff[0].x, boundingPointsDiff[0].y);
		// const bottomRight = new cv.Point(boundingPointsSum[lastIdx].x, boundingPointsSum[lastIdx].y);

		const pointsInRow = [];
		pointsToSearch.forEach((pt) => {
			// TODO: skip top left and top right point
			
			const distance = distanceFromLine(topLeft, topRight, pt);

			if (distance < 50) {
				pointsInRow.push(pt);
				cv.circle(grayscaleImg, new cv.Point(pt.x, pt.y), 3, [0, 255, 0, 255], 3);
			}
		});
		break;
	}

	// cv.circle(grayscaleImg, topLeft, 3, [0, 0, 0, 255], 3);
	// cv.circle(grayscaleImg, topRight, 3, [0, 0, 0, 255], 3);
	// cv.circle(grayscaleImg, bottomLeft, 3, [0, 0, 0, 255], 3);
	// cv.circle(grayscaleImg, bottomRight, 3, [0, 0, 0, 255], 3);
	// cv.line(
	// 	grayscaleImg,
	// 	topLeft,
	// 	new cv.Point(boundingPointsSum[0].x + 50, boundingPointsSum[0].y + 50),
	// 	[0, 255, 0, 255],
	// 	1
	// );

	postMessage({
		msg: "msg",
		payload: ""
	});

	// TODO: do some checking for grid size (N? M?) and make sure there is the correct amount of letters detected

	// TODO: prediction

	// ========================== RETURN + CLEANUP ==========================
	postMessage({ msg, payload: imageDataFromMat(grayscaleImg) });

	// contourImgBuffer.delete();
	img.delete();
	grayscaleImg.delete();
	contours.delete();
	hierarchy.delete();
}

/**
 * This function converts again from cv.Mat to ImageData
 */
function imageDataFromMat(mat) {
	// converts the mat type to cv.CV_8U
	const img = new cv.Mat();
	const depth = mat.type() % 8;
	const scale = depth <= cv.CV_8S ? 1.0 : depth <= cv.CV_32S ? 1.0 / 256.0 : 255.0;
	const shift = depth === cv.CV_8S || depth === cv.CV_16S ? 128.0 : 0.0;
	mat.convertTo(img, cv.CV_8U, scale, shift);

	// converts the img type to cv.CV_8UC4
	switch (img.type()) {
		case cv.CV_8UC1:
			cv.cvtColor(img, img, cv.COLOR_GRAY2RGBA);
			break;
		case cv.CV_8UC3:
			cv.cvtColor(img, img, cv.COLOR_RGB2RGBA);
			break;
		case cv.CV_8UC4:
			break;
		default:
			throw new Error("Bad number of channels (Source image must have 1, 3 or 4 channels)");
	}
	const clampedArray = new ImageData(new Uint8ClampedArray(img.data), img.cols, img.rows);
	return clampedArray;
}

/**
 * This exists to capture all the events that are thrown out of the worker
 * into the worker. Without this, there would be no communication possible
 * with the project.
 */
onmessage = async function (e) {
	switch (e.data.msg) {
		case "load": {
			try {
				// load opencv depending on wasm features available
				self.importScripts(paths.openCvWasmFeatureDetect);
				const opencvScript = await getOpenCV();
				self.importScripts(opencvScript);
				cv = await cv;

				// load tensorflow scripts
				self.importScripts(paths.tfjs);
				self.importScripts(paths.tfjsWasmBackend);
				tf.wasm.setWasmPaths({
					"tfjs-backend-wasm.wasm": paths.tfjsWasm,
					"tfjs-backend-wasm-simd.wasm": paths.tfjsWasmSimd,
					"tfjs-backend-wasm-threaded-simd.wasm": paths.tfjsWasmThreadedSimd
				});
				tf.setBackend("wasm");
				await tf.ready();
			} catch (err) {
				throw new Error(err);
			}

			postMessage({ msg: e.data.msg });
			break;
		}
		case "imageProcessing":
			return imageProcessing(e.data);
		default:
			break;
	}
};
